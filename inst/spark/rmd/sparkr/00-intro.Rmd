---
author: "Peter Diakumis"
date: "`r lubridate::date()`"
output:
  html_document:
    toc: true
description: "R and Spark"
title: "R and Spark"
---

```{r knitr_opts, include=F}
knitr::opts_chunk$set(
  echo = TRUE
)
```

## Intro

```{r pkgs}
# require(DBI)
require(dplyr)
require(arrow, include.only = "read_parquet")
# require(sessioninfo)
```

```{r sparkr_setup, message=F}
spark_home_pd <- file.path(Sys.getenv("SPARK_HOME"), "R", "lib")
require(SparkR, lib.loc = spark_home_pd)
SparkR::sparkR.session(master = "local[*]", sparkConfig = list(spark.driver.memory = "2g"))
```

## Create SparkDataFrame

- From R to Spark:

```{r}
rdf <- faithful
dplyr::glimpse(rdf)
sdf <- SparkR::as.DataFrame(rdf)
sdf |> head()
sdf |>
  SparkR::collect() |>
  head()
```

- Append to a Spark table:

```{r}
SparkR::write.df(sdf, path = "faithful.parquet", source = "parquet", mode = "append")
SparkR::write.df(sdf, path = "faithful.parquet", source = "parquet", mode = "append")
```

- Read directly into R from local filesystem with `{arrow}`:

```{r}
# need to specify full path
list.files("faithful.parquet", pattern = ".parquet$", full.names = T) |>
  arrow::read_parquet() |>
  str()
```

- Read directly into Spark from local filesystem with `{SparkR}`:

```{r}
# don't need to specify full path, recognised automatically
sdf2 <- SparkR::read.parquet("faithful.parquet")
sdf2 |>
  SparkR::collect() |>
  head()
sdf3 <- SparkR::read.parquet("faithful.parquet")
```
